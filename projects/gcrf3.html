<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link href="helpers/style.css" rel="stylesheet" type="text/css">
	<title>Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</title>
</head>
	<div id="Header" >
        <center>
				<h2>Deep Spatio-Temporal Random Fields for Efficient Video Segmentation</h2>
       To be presented at CVPR 2018, Salt Lake City, U.S.A.
        <h4><a href="https://siddharthachandra.github.io/">Siddhartha Chandra</a>, <a href="https://research.fb.com/people/couprie-camille/">Camille Couprie</a> &amp; <a href="http://www0.cs.ucl.ac.uk/staff/I.Kokkinos/">Iasonas Kokkinos</a></h4>
        </center>
	</div>
<div id="Logo" >
<a href="helpers/video-gcrf.pdf"><img src="helpers/paper-logo.png" width="45%"></a><!-- style="float:left;">-->
<a href="helpers/bibtex.txt"><img src="helpers/bib-logo.png" width="45%"></a><!-- style="float:left;">-->
</div>
<div id="MyBodyAbstract" >
<h2>Demo Video</h2>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/Jy5zVt7vpq4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
</center>
<h2>Abstract</h2>
<p>
In this work we introduce a time- and memory-efficient method for structured prediction that couples neuron decisions across both space at time. We show that we are able to perform exact and efficient inference on a densely-connected spatio-temporal graph by capitalizing on recent advances on deep Gaussian Conditional Random Fields (GCRFs).
</p>
<p>
Our method, called VideoGCRF is (a) efficient, (b) has a unique global minimum,
and (c) can be trained end-to-end alongside contemporary deep networks for video understanding.
We experiment with multiple connectivity patterns in the temporal domain, and present empirical improvements over strong baselines on the tasks of both semantic and instance segmentation of videos. </p>
<center>
<img src="helpers/video-teaser-cvpr2018-2.png" width="60%"> <!-- style="float:left;">-->
</center>
<p > <!--style="padding:30px 40px 30px 100px">-->
We jointly segment multiple images by passing them firstly through a fully convolutional network to obtain per-pixel class scores (<i>unary</i> terms <b>U</b>), alongside with spatial (<b>S</b>) and temporal (<b>T</b>) embeddings. We couple predictions at different spatial and temporal positions in terms of the inner product of their respective embeddings, shown here as arrows pointing to a graph edge. The final prediction is obtained by solving a linear system; this can eliminate spurious responses, e.g. on the left pavement, by diffusing the per-pixel node scores over the whole spatio-temporal graph. The CRF and CNN architecture is jointly trained end-to-end, while CRF inference is exact and particularly efficient.</p>
<h2>Visualization of Pairwise Affinities</h2>
<center>
<img src="helpers/camel.png" width="40%"> <!-- style="float:left;">-->
</center>
<p>
Visualization of instance segmentation through VideoGCRF: In row 1 we focus on a single point of the CRF graph, shown as a cross, and show as a heatmap its spatial (inter-frame) and temporal (intra-frame) affinities to all other graph nodes. In row 2 we show the predictions that would be obtained by frame-by-frame segmentation, relying exclusively on the FCN's unary terms, while in row 3 we show the results obtained after solving the VideoGCRF inference problem. We observe that in frame-by-frame segmentation a second camel is incorrectly detected due to its similar appearance properties. However, VideoGCRF inference  exploits temporal context and  focuses solely on the correct object.</p>
<h2>Results on the CamVid Dataset</h2>
<center>
<img src="helpers/table.png" width="60%"> <!-- style="float:left;">-->
<p style="padding-bottom:100px;">
We compare our results with some of the previously published methods, as well as our
own implementation of the ResNet-101 network which serves as our base network.</p>
</center>
</div>
<br><br>
</body></html>
